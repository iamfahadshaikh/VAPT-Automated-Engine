# Phase 1-4 Stabilization Plan

**Date**: 2026-01-12  
**Objective**: Complete and stabilize Phases 1-4 ONLY. NO Phase 5.  
**Status**: üü° In Progress

---

## Executive Summary

**Current Reality**:
- ‚úÖ Phase 1 (Discovery): Architecturally sound, needs hardening
- ‚ö†Ô∏è Phase 2 (Crawling): **CRITICAL GAP** - Katana exists but not integrated properly
- ‚ö†Ô∏è Phase 3 (Payload Testing): Tools exist but lack crawler-driven inputs
- ‚ö†Ô∏è Phase 4 (Correlation): Engines exist but inputs are shallow

**Root Cause**: Built Phase 4 "production maturity" features before Phase 2-3 foundations were solid.

**Solution**: Fix in strict order: Phase 2 ‚Üí Phase 3 ‚Üí Phase 4 ‚Üí Phase 1 hardening

---

## üö´ Hard Constraints (DO NOT VIOLATE)

- ‚ùå NO Phase 5 (traffic replay, regression, etc.)
- ‚ùå NO UI/dashboards/frontend
- ‚ùå NO random tool additions without gating logic
- ‚ùå NO payload tools without crawler-verified inputs
- ‚ùå NO marking SKIPPED tools as success

---

## üìã Stabilization Checklist

### Phase 2: Crawling & Expansion (CRITICAL - DO FIRST)

**Current State**:
- ‚úÖ `katana_crawler.py` exists (302 lines)
- ‚úÖ `light_crawler.py` exists (fallback)
- ‚úÖ `crawler_integration.py` exists
- ‚ùå NOT integrated as mandatory Phase 2 step
- ‚ùå NOT populating `DiscoveryCache` properly
- ‚ùå NOT used by payload tools

**Required Fixes** (Priority 1):

- [ ] **Make crawler mandatory**
  - [ ] Crawler MUST run before any payload tool
  - [ ] No payload tool may run without crawler confirmation
  - [ ] Crawler failure = BLOCK payload phase

- [ ] **Fix crawler integration**
  - [ ] Ensure `katana_crawler.py` populates:
    - [ ] `endpoints` (Set[str])
    - [ ] `parameters` (Dict[str, Set[str]])
    - [ ] `forms` (List[Dict])
    - [ ] `api_endpoints` (List[str])
  - [ ] Verify crawler executes JavaScript
  - [ ] Track discovery source per endpoint
  - [ ] Set confidence levels

- [ ] **Populate DiscoveryCache**
  - [ ] `DiscoveryCache.endpoints` = crawler.endpoints
  - [ ] `DiscoveryCache.params` = crawler.parameters
  - [ ] `DiscoveryCache.forms` = crawler.forms
  - [ ] `DiscoveryCache.api_endpoints` = crawler.api_endpoints
  - [ ] Mark all as `source="crawler"`

- [ ] **Validate EndpointGraph**
  - [ ] `EndpointGraph.add_crawl_result()` called for each endpoint
  - [ ] Graph tracks: endpoint ‚Üí method ‚Üí params ‚Üí sources
  - [ ] Graph is finalized before payload tools run
  - [ ] Graph queries work:
    - [ ] `get_reflectable_endpoints()`
    - [ ] `get_injectable_sql_endpoints()`
    - [ ] `get_parametric_endpoints()`

**Success Criteria**:
- ‚úÖ Crawler runs first, produces endpoints + params + forms
- ‚úÖ EndpointGraph built from crawler data
- ‚úÖ No payload tool runs without crawler confirmation

**Files to Fix**:
- `phase2_pipeline.py` - Make crawler mandatory
- `katana_crawler.py` - Ensure proper output parsing
- `endpoint_graph.py` - Validate graph building
- `cache_discovery.py` - Ensure crawler populates cache

---

### Phase 3: Payload-Driven Testing (DO SECOND)

**Current State**:
- ‚úÖ Payload tools exist: `dalfox`, `sqlmap`, `commix`, etc.
- ‚ùå NOT gated by crawler results
- ‚ùå NOT using EndpointGraph for targeting
- ‚ùå Inputs are weak/random

**Required Fixes** (Priority 2):

- [ ] **Strict Gating**
  - [ ] Dalfox runs ONLY on `graph.get_reflectable_endpoints()`
  - [ ] SQLMap runs ONLY on `graph.get_injectable_sql_endpoints()`
  - [ ] Commix runs ONLY on `graph.get_injectable_cmd_endpoints()`
  - [ ] Nuclei runs on all endpoints (broad scan)

- [ ] **Input Verification**
  - [ ] Each payload tool receives:
    - [ ] Endpoint URL
    - [ ] Parameter list (from crawler)
    - [ ] Method (GET/POST)
    - [ ] Confidence score
  - [ ] No blind fuzzing
  - [ ] No spray-and-pray

- [ ] **Payload Attempt Tracking**
  - [ ] Log each payload sent
  - [ ] Track success vs failure ratio
  - [ ] Record evidence (reflection, execution)

- [ ] **Finding Quality**
  - [ ] Each finding must include:
    - [ ] Entry point (endpoint + parameter)
    - [ ] Payload used
    - [ ] Evidence (response snippet)
    - [ ] Confidence score
    - [ ] Tool name

**Success Criteria**:
- ‚úÖ Payload tools run ONLY on crawler-verified endpoints
- ‚úÖ Each payload has clear entry point and evidence
- ‚úÖ No random fuzzing

**Files to Fix**:
- `strict_gating_loop.py` - Enforce crawler-driven gating
- Individual tool wrappers (dalfox, sqlmap, etc.)
- `decision_ledger.py` - Ensure prerequisites check crawler data

---

### Phase 4: Correlation, Risk, Confidence (DO THIRD)

**Current State**:
- ‚úÖ `finding_correlator.py` exists (Phase 3)
- ‚úÖ `risk_engine.py` exists (Phase 3)
- ‚úÖ `api_discovery.py` exists (Phase 3)
- ‚ùå Inputs are shallow (not using crawler depth, JS visibility, etc.)
- ‚ùå Confidence scores lack context

**Required Fixes** (Priority 3):

- [ ] **Enhance Confidence Scoring**
  - [ ] Consider:
    - [ ] Crawl depth (deeper = less confident)
    - [ ] JS execution visibility
    - [ ] Payload confirmation (reflected? executed?)
    - [ ] Reproducibility (same result twice?)
  - [ ] Multi-tool agreement increases confidence

- [ ] **Risk Engine**
  - [ ] Map findings to OWASP Top 10
  - [ ] Aggregate duplicate issues
  - [ ] De-duplicate across tools
  - [ ] Calculate risk score based on:
    - [ ] Severity
    - [ ] Confidence
    - [ ] Exploitability
    - [ ] Business impact

- [ ] **Reports**
  - [ ] Clearly separate:
    - [ ] Informational (confidence < 30%)
    - [ ] Suspected (confidence 30-70%)
    - [ ] Confirmed (confidence > 70%)
  - [ ] Explain risk scores
  - [ ] Show tool agreement

- [ ] **CI Output**
  - [ ] Deterministic
  - [ ] Fail ONLY on confirmed findings (confidence > 70%)
  - [ ] Warn on suspected findings

**Success Criteria**:
- ‚úÖ Confidence scores are explainable
- ‚úÖ Risk engine considers crawler context
- ‚úÖ Reports tell coherent story

**Files to Fix**:
- `finding_correlator.py` - Use crawler depth/JS data
- `risk_engine.py` - Enhance confidence calculation
- `ci_integration.py` - Fail only on confirmed findings

---

### Phase 1: Recon & Decision Engine (DO LAST - HARDENING)

**Current State**:
- ‚úÖ Architecturally correct
- ‚úÖ Signal-driven
- ‚úÖ Budget-aware
- ‚ö†Ô∏è Needs hardening, not expansion

**Required Fixes** (Priority 4):

- [ ] **Tool Audit**
  - [ ] Each tool has ONE clear purpose
  - [ ] Each tool produces ONE signal type
  - [ ] No tool runs "just because"

- [ ] **Signal Clarity**
  - [ ] Every tool execution explains WHY it ran
  - [ ] `EXECUTED_NO_SIGNAL` is NOT success
  - [ ] `failure_reason` enums normalized

- [ ] **HTTPS Probe Immutability**
  - [ ] Result is immutable post-planning
  - [ ] No re-probing during execution

- [ ] **DiscoveryCache Authority**
  - [ ] DiscoveryCache is ONLY source of truth
  - [ ] No direct cache bypass

**Success Criteria**:
- ‚úÖ Clean execution graph
- ‚úÖ Zero ambiguity in tool decisions
- ‚úÖ All signals traceable

**Files to Audit**:
- `decision_ledger.py`
- Individual tool wrappers
- `cache_discovery.py`

---

## üéØ Implementation Order

### Week 1: Phase 2 Stabilization (CRITICAL)

**Day 1-2**: Crawler Integration
- Fix `katana_crawler.py` output parsing
- Ensure crawler populates `DiscoveryCache`
- Make crawler mandatory in `phase2_pipeline.py`

**Day 3-4**: EndpointGraph Validation
- Verify `endpoint_graph.py` builds from crawler data
- Test graph queries
- Ensure graph is finalized before payload phase

**Day 5**: Testing
- Run full Phase 2 test suite
- Validate crawler ‚Üí graph ‚Üí cache flow
- Document gaps

### Week 2: Phase 3 Stabilization

**Day 1-2**: Strict Gating
- Fix `strict_gating_loop.py` to use crawler data
- Ensure Dalfox runs ONLY on reflectable endpoints
- Ensure SQLMap runs ONLY on SQL-injectable endpoints

**Day 3-4**: Input Verification
- Update payload tool wrappers to accept crawler inputs
- Add payload attempt tracking
- Improve finding quality

**Day 5**: Testing
- Run full Phase 3 test suite
- Validate gating logic
- Document gaps

### Week 3: Phase 4 & 1 Stabilization

**Day 1-2**: Confidence & Risk
- Enhance confidence scoring (crawler depth, JS visibility)
- Fix risk engine to use crawler context
- Update reports

**Day 3**: CI Output
- Make CI output deterministic
- Fail only on confirmed findings

**Day 4-5**: Phase 1 Hardening
- Audit tool purposes
- Normalize signals
- Fix ambiguities

---

## üîç Validation Checklist (MANDATORY)

Before declaring completion:

- [ ] ‚úÖ Crawler drives payload scope
- [ ] ‚úÖ No payload tool runs without crawl evidence
- [ ] ‚úÖ No skipped tool is marked successful
- [ ] ‚úÖ OWASP mapping is explicit
- [ ] ‚úÖ Confidence scores are explainable
- [ ] ‚úÖ Reports tell a coherent story
- [ ] ‚úÖ Phase boundaries are respected

---

## üõë Explicit Non-Goals (DO NOT TOUCH)

- ‚ùå No auto-exploitation
- ‚ùå No credential stuffing
- ‚ùå No brute-force auth
- ‚ùå No DoS testing
- ‚ùå **NO PHASE 5** (traffic replay, regression, etc.)

---

## üìÅ Key Files to Focus On

### Phase 2 (Crawling)
- `katana_crawler.py` - Main crawler
- `light_crawler.py` - Fallback
- `crawler_integration.py` - Integration layer
- `endpoint_graph.py` - Graph builder
- `cache_discovery.py` - Cache population
- `phase2_pipeline.py` - Orchestration

### Phase 3 (Payload Testing)
- `strict_gating_loop.py` - Gating logic
- Individual tool wrappers (dalfox, sqlmap, commix, etc.)
- `decision_ledger.py` - Prerequisites

### Phase 4 (Correlation)
- `finding_correlator.py` - Multi-tool dedup
- `risk_engine.py` - Risk scoring
- `api_discovery.py` - API detection

### Phase 1 (Discovery)
- All recon tools
- `cache_discovery.py`
- `decision_ledger.py`

---

## üé¨ End State Definition

**When done correctly**:
- ‚úÖ Crawler is mandatory Phase 2 step
- ‚úÖ EndpointGraph built from crawler data
- ‚úÖ Payload tools gated by crawler evidence
- ‚úÖ Confidence scores consider crawler context
- ‚úÖ Reports are explainable and coherent
- ‚úÖ All phase boundaries respected

**Result**: A professional, architecture-driven vulnerability assessment engine with stateful crawling and targeted attack simulation.

---

**Status**: üü° In Progress  
**Next Action**: Fix Phase 2 crawler integration  
**Blocker**: None  
**ETA**: 3 weeks for full stabilization
